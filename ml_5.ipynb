{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hgs8WEcGRQm"
      },
      "source": [
        "# Assignment 5\n",
        "## Decision Trees and Random Forests for Regression\n",
        "\n",
        "### About this notebook\n",
        "\n",
        "The notebook has two parts, as has the assignment, the first part being a \"tutorial like\" walkthrough, the second asking you to implement a regression tree yourself. Hence, when going through the first part, remember that you need to have time left to actually work with the second!\n",
        "\n",
        "Both parts of the assignment can be handled within this notebook, however, the ID3-implementation should be done in a separate Python-script (ID3_reg.py). Feel free to move the second part to its own notebook for better overview. In any case, if you make changes in the Python file, you might need to restart the kernel for the notebook that is using it, so that changes get properly loaded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JesgWuYGRQp"
      },
      "outputs": [],
      "source": [
        "# YOU DON'T HAVE TO RUN THIS IF EVERYTHING IS ALREADY INSTALLED CORRECTLY\n",
        "# !pip3 install --upgrade pip\n",
        "# !pip3 install graphviz\n",
        "# !pip3 install dtreeviz\n",
        "# !pip3 install numpy scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUZV1epRGRQr"
      },
      "source": [
        "# Part 1\n",
        "\n",
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-sHtfYDGRQs"
      },
      "source": [
        "First we load the dataset. Ultimately, you should be working with the California housing data, but for quicker test runs, it might help to first start out with the Diabetes data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOZzpdvzGRQs"
      },
      "outputs": [],
      "source": [
        "#run time 0.8s\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from ConceptDataRegr import ConceptDataRegr\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# test_case = 'diabetes'\n",
        "test_case = 'california'\n",
        "\n",
        "if test_case == 'california':\n",
        "    dataset = fetch_california_housing()\n",
        "elif test_case == 'diabetes': \n",
        "    dataset = load_diabetes()\n",
        "else:\n",
        "    raise ValueError('Unknown test case')\n",
        "\n",
        "X = dataset.data\n",
        "y = dataset.target\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.data[0]"
      ],
      "metadata": {
        "id": "ezKvqZ7mI2bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpzBM8wQGRQt"
      },
      "source": [
        "Some information about the dataset you're looking at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaHArRDxGRQt"
      },
      "outputs": [],
      "source": [
        "if test_case == 'california' :\n",
        "    print(\"target:\", list(dataset.target_names))\n",
        "print(\"features:\", list(dataset.feature_names))\n",
        "print(\"description:\", dataset.DESCR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hfNBN1GGRQu"
      },
      "source": [
        "Split it into train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l-Y-wI-GRQu"
      },
      "outputs": [],
      "source": [
        "#run time 0.7s\n",
        "\n",
        "train_ratio = 0.70\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "X = dataset.data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJB71n4tGRQv"
      },
      "source": [
        "What is the reason why you would like to have validation and test set and not just a test set?\n",
        "Answer:\n",
        "\n",
        "\n",
        "*   In order to use the validation set to get an estimate of the model skill while tuning the hyperparameters of the model. The validation set is not used to train the model. However, the reason why the test set is not used instead is because the test set can cause a **biased estimation of the model skill**. Thus, we have a set that is not part of the process in training the model to make the **estimation unbiased**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "More guiding questions:\n",
        "How can you use the validation set?\n",
        "* You can use it for to get an unbiased evaluation of a model fit on the training dataset while at the same time, tuning the hyperparameters.\n",
        "\n",
        "Can you use the validation set in some way to tune the hyperparameters of your model?\n",
        "Can you use the test set to tune the hyperparameters of your model?\n",
        "* You can use the validation set to evaluate the skill of the model while training the model. Afterwards, when the training is complete, you can evaluate based on the test set instead and then make adjustments to the parameters in order to get a better model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTSUqndaGRQv"
      },
      "source": [
        "## Decision Tree Regressor\n",
        "\n",
        "Run the cells below and inspect the output. Use the documentation where needed. Be prepared to answer questions posed by the TA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FydcosVGRQw"
      },
      "outputs": [],
      "source": [
        "#run time 0.7s\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "regressor1 = DecisionTreeRegressor(random_state=0) # nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE2UUrb-GRQx"
      },
      "source": [
        "Now let's examine the decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SnPdQ1aGRQx"
      },
      "outputs": [],
      "source": [
        "#run time 1.8s\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cross_val_score(regressor1, X_train, y_train, cv=10) #Array of scores of the estimator for each run of the cross validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TkHyyPVGRQy"
      },
      "outputs": [],
      "source": [
        "#run time 0.3s\n",
        "\n",
        "regressor1.fit(X_train, y_train) #Build a decision tree regressor from the training set (X, y)\n",
        "regressor1.score(X_test, y_test) #Return the coefficient of determination of the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm1QUe7_GRQ0"
      },
      "source": [
        "What do these numbers mean?\n",
        "Check out [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
        "and [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=decision+tree)\n",
        "to find the answers.\n",
        "\n",
        "How do you interpret the numbers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhpf6V1qGRQ1"
      },
      "source": [
        "Let's have a look at two other parameters, max_depth and min_samples_leaf.\n",
        "How do you interpret the following numbers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pASAvBdnGRQ1"
      },
      "source": [
        "## Decision Tree Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wt_A-9XGRQ1"
      },
      "outputs": [],
      "source": [
        "#run time 0.2s\n",
        "\n",
        "regressor2 = DecisionTreeRegressor(max_depth=1, random_state=0) # The maximum depth of the tree.\n",
        "cross_val_score(regressor2, X_train, y_train, cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLnIn88FGRQ2"
      },
      "outputs": [],
      "source": [
        "#run time 0.1s\n",
        "\n",
        "regressor2.fit(X_train, y_train)\n",
        "regressor2.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRa6R_IFGRQ2"
      },
      "outputs": [],
      "source": [
        "#run time 1.2s\n",
        "\n",
        "regressor3 = DecisionTreeRegressor(min_samples_leaf=20, random_state=0) # The minimum number of samples required to split an internal node\n",
        "cross_val_score(regressor3, X_train, y_train, cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meyBJ-jaGRQ3"
      },
      "outputs": [],
      "source": [
        "#run time 0.1s\n",
        "\n",
        "regressor3.fit(X_train, y_train)\n",
        "regressor3.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor3.score(X_val, y_val)"
      ],
      "metadata": {
        "id": "N6RDPe1w24Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binaryRegressor(depth, samples):\n",
        "  regressor = DecisionTreeRegressor(max_depth = depth, min_samples_leaf=samples, random_state=0)\n",
        "  regressor.fit(X_train, y_train)\n",
        "  return regressor"
      ],
      "metadata": {
        "id": "sINBg95MwSyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "\n",
        "for depth in range(1, 20):\n",
        "  for samples in range(2, 100):\n",
        "    scores.append([(depth, samples), binaryRegressor(depth, samples).score(X_val, y_val)])"
      ],
      "metadata": {
        "id": "SNzxYewfw6bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def takeSecond(x):\n",
        "  return x[1]\n",
        "\n",
        "scores.sort(key=takeSecond, reverse=True)"
      ],
      "metadata": {
        "id": "6euRTmPtzObI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores[:10]"
      ],
      "metadata": {
        "id": "d6HWmlHhchUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binaryRegressor(14, 6).score(X_test, y_test)"
      ],
      "metadata": {
        "id": "soUWKWgmdEmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaR-fntSGRQ3"
      },
      "source": [
        "## Decision Tree Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMdthZgWGRQ4"
      },
      "source": [
        "The next cells give examples how to visualize regressor2 and regressor3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1u1-hBOGRQ4"
      },
      "outputs": [],
      "source": [
        "#run time 0.2s\n",
        "\n",
        "from sklearn import tree\n",
        "import graphviz\n",
        "from IPython.display import Image\n",
        "\n",
        "dot_data = tree.export_graphviz(regressor2, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph.render(\"decision_tree_regressor2\")\n",
        "Image(\"decision_tree_regressor2.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq_0y-yyGRQ4"
      },
      "outputs": [],
      "source": [
        "#run time 4.8s\n",
        "\n",
        "dot_data = tree.export_graphviz(regressor3, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph.render(\"decision_tree_regressor3\")\n",
        "Image(\"decision_tree_regressor3.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfI2wHbyGRQ5"
      },
      "source": [
        "Another nice way to visualize the decision trees nicely is with dtreeviz. To make these plots it takes quite some time, so we recommend to use this visualization tool for trees with few nodes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh7Rge6ZGRQ5"
      },
      "outputs": [],
      "source": [
        "# run time 6.9s\n",
        "\n",
        "from dtreeviz.trees import dtreeviz\n",
        "\n",
        "viz = dtreeviz(regressor2, X, y,\n",
        "                target_name=\"target\",\n",
        "                feature_names=dataset.feature_names)\n",
        "#viz.view()\n",
        "# this opens the visualization in a new window. If you want to display the output inside the notebook use:\n",
        "viz\n",
        "# if you want to store the output in a file use:\n",
        "#viz.save(\"dtreeviz.svg\")\n",
        "# instead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67UcuWP-GRQ6"
      },
      "source": [
        "## Explainability\n",
        "\n",
        "If you want to visualize (explain) the decision path for one prediction, you can also use dtreeviz:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNR74TwEGRQ6"
      },
      "outputs": [],
      "source": [
        "# run time 6.8s\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "sample = X_test[np.random.randint(0, len(X_test)),:] # random sample from training\n",
        "\n",
        "viz = dtreeviz(regressor2, X, y,\n",
        "                target_name=\"target\",\n",
        "                feature_names=dataset.feature_names,\n",
        "                X=sample)\n",
        "#viz.view()\n",
        "viz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgPxrWV6GRQ6"
      },
      "source": [
        "For bigger graphs you just show the decision path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MCfVXjQGRQ7"
      },
      "outputs": [],
      "source": [
        "# run time 10.4s\n",
        "\n",
        "viz = dtreeviz(regressor3, X, y,\n",
        "                target_name=\"target\",\n",
        "                feature_names=dataset.feature_names,\n",
        "                X=sample,\n",
        "                show_just_path=True)\n",
        "#viz.view()\n",
        "viz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVqVF3aqGRQ7"
      },
      "source": [
        "Another option to explain the prediction for big trees is this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8e5Tw7fGRQ7"
      },
      "outputs": [],
      "source": [
        "# run time 0.1s\n",
        "\n",
        "from dtreeviz.trees import explain_prediction_path\n",
        "\n",
        "print(explain_prediction_path(regressor3, sample, feature_names=dataset.feature_names, explanation_type=\"plain_english\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCgSPMJRGRQ8"
      },
      "source": [
        "## Pruning\n",
        "\n",
        "### Cost Complexity Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CBD5wL-GRQ8"
      },
      "source": [
        "A smart way of pruning is to use cost complexity pruning. This method is based on the idea that a tree with a lot of nodes is more likely to overfit than a tree with few nodes. Therefore, we can prune the tree by removing nodes that are not important for the prediction. The cost complexity pruning method uses a parameter $\\alpha$ to determine how many nodes to remove. It basically is a tradeoff between having a tree with many nodes that has a small total MSE, vs. a tree with fewer nodes but greater total MSE. The following code shows how to use the cost complexity pruning method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc96Wo-_GRQ8"
      },
      "source": [
        "We find the alphas that change the Decision Tree to be \"cut down\" and we record the worsening of the MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HxnYgwVGRQ8"
      },
      "outputs": [],
      "source": [
        "# run time 0.8s\n",
        "\n",
        "path = regressor1.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXtGelDTGRQ8"
      },
      "source": [
        "We can then plot the MSE for each $\\alpha$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg4GcBeWGRQ9"
      },
      "outputs": [],
      "source": [
        "# run time 0.4s\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax.set_xlabel(\"effective alpha\")\n",
        "ax.set_ylabel(\"total impurity of leaves\")\n",
        "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1EifXB-GRQ9"
      },
      "source": [
        "You can now train a Decision Tree for each $\\alpha$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSGOYaYPGRQ9"
      },
      "outputs": [],
      "source": [
        "# run time 0.2s\n",
        "\n",
        "regressors = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    regressor = DecisionTreeRegressor(min_samples_leaf=20, random_state=0, ccp_alpha=ccp_alpha)\n",
        "    regressor.fit(X_train, y_train)\n",
        "    regressors.append(regressor)\n",
        "print(\n",
        "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
        "        regressors[-1].tree_.node_count, ccp_alphas[-1]\n",
        "    ),\n",
        ")\n",
        "if regressors[-1].tree_.node_count == 1:\n",
        "    print(\"Removing last node.\")\n",
        "    regressors = regressors[:-1]\n",
        "    ccp_alphas = ccp_alphas[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKK52N2EGRQ9"
      },
      "outputs": [],
      "source": [
        "# run time 0.5s\n",
        "\n",
        "node_counts = [regressor.tree_.node_count for regressor in regressors]\n",
        "depth = [regressor.tree_.max_depth for regressor in regressors]\n",
        "fig, ax = plt.subplots(2, 1)\n",
        "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax[0].set_xlabel(\"alpha\")\n",
        "ax[0].set_ylabel(\"number of nodes\")\n",
        "ax[0].set_title(\"Number of nodes vs alpha\")\n",
        "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax[1].set_xlabel(\"alpha\")\n",
        "ax[1].set_ylabel(\"depth of tree\")\n",
        "ax[1].set_title(\"Depth vs alpha\")\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-dtEOeuGRQ9"
      },
      "source": [
        "This is a way to get all the scores for each tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBedQ3ahGRQ-"
      },
      "outputs": [],
      "source": [
        "# run time 0.6s\n",
        "\n",
        "train_scores = [regressor.score(X_train, y_train) for regressor in regressors]\n",
        "val_scores = [regressor.score(X_val, y_val) for regressor in regressors]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and validation sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, val_scores, marker=\"o\", label=\"val\", drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8I9k3EfGRQ-"
      },
      "source": [
        "The best tree is the one with the highest score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRhxfy_dGRQ-"
      },
      "outputs": [],
      "source": [
        "# run time 0.8s\n",
        "\n",
        "idx_max = np.argmax(val_scores)\n",
        "regressor_best = regressors[idx_max]\n",
        "print(\"Best alpha: {}\".format(ccp_alphas[idx_max]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDMmS3odGRQ_"
      },
      "outputs": [],
      "source": [
        "# run time 0.7s\n",
        "\n",
        "regressor_best.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdPmJ0_8GRQ_"
      },
      "outputs": [],
      "source": [
        "# run time 0.8s\n",
        "\n",
        "dot_data = tree.export_graphviz(regressor_best, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph.render(\"decision_tree_regressor_best\")\n",
        "Image(\"decision_tree_regressor_best.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDjCvSVdGRQ_"
      },
      "source": [
        "## Ensemble methods: \n",
        "\n",
        "Experiment with at least two methods. Inspect the documentation of the different estimators. Note that you can use regressors as estimators within an ensemble that are themselves based on an ensemble. Below is an example for a (tiny) voting ensemble. Visualise your results to be able to discuss them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySGnFXV-GRQ_"
      },
      "outputs": [],
      "source": [
        "# run time 1.2s\n",
        "\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "voting=VotingRegressor(estimators=[('lr', LinearRegression()), ('dt', DecisionTreeRegressor())])\n",
        "voting.fit(X_train, y_train)\n",
        "voting.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37U8FjypGRRA"
      },
      "source": [
        "## Boosting!\n",
        "\n",
        "Experiment with an AdaBoostRegressor and interpret the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfb7Y8BvGRRA"
      },
      "outputs": [],
      "source": [
        "# run time 0.2s\n",
        "\n",
        "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "boosting = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=number_of_trees, random_state=0)\n",
        "boosting.fit(X_train, y_train)\n",
        "boosting.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMrcJBHXGRRA"
      },
      "outputs": [],
      "source": [
        "# run time 1m 13s to 3m\n",
        "\n",
        "fig, ax = plt.subplots(5,2)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.set_title(\"Tree {}\".format(i))\n",
        "    tree.plot_tree(boosting.estimators_[i], ax=axi, feature_names=dataset.feature_names, filled=True, rounded=True)\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx5jg6VmGRRB"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "Experiment with different parameters for the RF-Regressor. Test at least two different parameter sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf-HoXebGRRB"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "number_of_trees = 10\n",
        "forest = RandomForestRegressor(n_estimators=number_of_trees, random_state=0)\n",
        "forest.fit(X_train, y_train)\n",
        "forest.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-OB3jYkGRRB"
      },
      "outputs": [],
      "source": [
        "for treeid in range(number_of_trees):\n",
        "    dot_data = tree.export_graphviz(forest.estimators_[treeid], feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
        "    graph = graphviz.Source(dot_data, format=\"png\") \n",
        "    graph.render(\"forest_treeid\"+str(treeid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCfrZl03GRRB"
      },
      "outputs": [],
      "source": [
        "# run time 1m 23s to 3m\n",
        "\n",
        "fig, ax = plt.subplots(5,2)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.set_title(\"Tree {}\".format(i))\n",
        "    tree.plot_tree(forest.estimators_[i], ax=axi, feature_names=dataset.feature_names, filled=True, rounded=True)\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiBvgB4GRRC"
      },
      "source": [
        "# Part 2\n",
        "\n",
        "Implement your own version of a regression tree by using the provided code skeleton which you can find in ID3_reg.py. If you prefer to implement your own tree entirely, this is fine, but you should be confident in handling the implementation of the recursion properly. Please note: the actual task description (which evaluations to provide and discuss) is given in Canvas, as is a check list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3zSd3a-GRRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0b3348-2d22-460c-b04f-4b3feead853b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ordered_set\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: ordered-set\n",
            "Successfully installed ordered-set-4.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ordered_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH-pMktCGRRC"
      },
      "source": [
        "## Discretizing the data\n",
        "\n",
        "To make experiments with categorical data, you need to discretize the data (this goes both for the 'california' and the 'diabetes' cases). In order to have the entire data set \"as is\" for the binning, you can prepare a binning rule on the original data (X, if you continue to work with the data from part 1), that you then apply to your train and test data sets. To make sure that you do not miss any possible attribute values, use the entire set (X) again when providing the categorical values (here bin indices) to the ID3 tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58IS0474GRRC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import numpy as np\n",
        "\n",
        "# you might want to try different numbers of bins\n",
        "# inspect the data set description in part 1 to find suitable numbers \n",
        "if test_case == 'california':\n",
        "    bins = [5,4,2,2,2,2,16,15]\n",
        "    #bins = [2, 2, 2, 2, 2, 2, 2, 2]\n",
        "elif test_case == 'diabetes':\n",
        "    bins = [2,2,2,2,2,2,2,2,2,2]\n",
        "\n",
        "# here you can test to use different strategies, see the KBinsDiscretizer documentation\n",
        "binner = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='kmeans')\n",
        "binning_rule = binner.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxkFDXXvGRRD"
      },
      "source": [
        "## Creating and evaluating an ID3-based Regression Tree\n",
        "\n",
        "The following cells provide some framework for creating / testing your own, ID3-based, regressor. To see that your tree is constructed correctly, some prints are provided (essentially from the lecture) within the handout directory, that are created with the \"ConceptData\" from the lecture. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpSYAqIZGRRD"
      },
      "outputs": [],
      "source": [
        "import ID3_reg\n",
        "import ConceptDataRegr as cd\n",
        "import graphviz\n",
        "# For testing that you get the correct output from your own implementation, use the \"ConceptData\" data set:\n",
        "#test_case = 'concept'\n",
        "\n",
        "test_case = 'california'\n",
        "\n",
        "if test_case == 'concept' :\n",
        "    attributes, binned_X_train, y_train, binned_X_test, y_test = cd.ConceptDataRegr().get_data()\n",
        "    binned_X_val = []\n",
        "    y_val = []\n",
        "     \n",
        "else :\n",
        "    # if running on \"real\" data, you now need to use the binning_rule you computed above \n",
        "    binned_X = binning_rule.transform(X).astype(int)\n",
        "    binned_X_train = binning_rule.transform(X_train).astype(int)\n",
        "    binned_X_val = binning_rule.transform(X_val).astype(int)\n",
        "    binned_X_test = binning_rule.transform(X_test).astype(int)\n",
        "\n",
        "    attributes = {}\n",
        "    i = 0\n",
        "    for attr in dataset.feature_names :\n",
        "        attributes[attr] = set(binned_X[:,i])\n",
        "        i+=1\n",
        "\n",
        "#print(attributes)\n",
        "\n",
        "# Now, set up the tree (inspect the ID3_reg class!)   \n",
        "id3 = ID3_reg.ID3RegressionTreePredictor()\n",
        "\n",
        "# visualising in the \"bubble\" format from the lecture\n",
        "myTree = id3.fit(binned_X_train, y_train, attributes)\n",
        "# dot_data = id3.makeDotData().source\n",
        "# graph = graphviz.Source(dot_data, format=\"pdf\")\n",
        "# graph.render(test_case+\"_bubbles\")\n",
        "# graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR6wGEsPGRRD"
      },
      "outputs": [],
      "source": [
        "#predicted = id3.predict(binned_X_val)\n",
        "#print(predicted)\n",
        "\n",
        "# As of now, the ID3_reg class does only provide a stubb of a score-method - please implement one according to\n",
        "# the description of DecisionTreeRegressor.score() for easier comparison with the scikit-learn trees!\n",
        "print(id3.score(binned_X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multivariateRegressor(depth, samples):\n",
        "  regressor = ID3_reg.ID3RegressionTreePredictor(minSamplesSplit = samples, maxDepth = depth)\n",
        "  regressor.fit(binned_X_train, y_train, attributes)\n",
        "  return regressor"
      ],
      "metadata": {
        "id": "2N8t4DbG4mBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = ID3_reg.ID3RegressionTreePredictor(minSamplesSplit = , maxDepth = 10)\n",
        "reg.fit(binned_X_train, y_train, attributes)\n",
        "print(reg.score(binned_X_val, y_val))"
      ],
      "metadata": {
        "id": "RGFlT0XkJ4_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96f86bb-6ba6-4a6e-851c-d0f78fdf6bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6160894387672671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mulScores = []\n",
        "\n",
        "for depth in range(1, 20):\n",
        "  for samples in range(2, 100):\n",
        "    mulScores.append([(depth, samples), multivariateRegressor(depth, samples).score(binned_X_val, y_val)])"
      ],
      "metadata": {
        "id": "V9d4PEmo5dcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def takeSecond(x):\n",
        "  return x[1]\n",
        "\n",
        "mulScores.sort(key=takeSecond, reverse=True)"
      ],
      "metadata": {
        "id": "ZpnweDN0GpJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mulScores[:10]"
      ],
      "metadata": {
        "id": "jh3eqiaaGuTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "justScores = [x[1] for x in mulScores]"
      ],
      "metadata": {
        "id": "ukY0xjtVHVao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xI56HgTGRRD"
      },
      "outputs": [],
      "source": [
        "# printing to squares if you want a tree that looks more like those from part 1 ;-)\n",
        "dot_data_pretty='digraph Tree {\\n'+\\\n",
        "    'node [shape=box'+\\\n",
        "    ', style=\"rounded\", color=\"black\"'+\\\n",
        "    ', fontname=\"helvetica\"] ;\\n'+\\\n",
        "    'graph [ranksep=equally, splines=polyline] ;\\n'+\\\n",
        "    'edge [fontname=\"helvetica\"] ;\\n'+\\\n",
        "    dot_data[9:]\n",
        "\n",
        "graph = graphviz.Source(dot_data_pretty, format=\"png\")\n",
        "graph.render(test_case+\"_pretty\")\n",
        "#graph.view()\n",
        "graph"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "67UcuWP-GRQ6",
        "mCgSPMJRGRQ8",
        "mDjCvSVdGRQ_",
        "37U8FjypGRRA",
        "hx5jg6VmGRRB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "63f08e0be87bb1ec22e3a665002567369c2bb78585d8d1135c35fb08381ea5a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}